摘要
动机：图像文本对不用标注数据，可以大规模扩展数据；多模态特征更容易进行零样本迁移学习
构建4亿图文配对数据集，采用多模态的对比学习方法进行模型预训练。
预训练后的模型使用自然语言（prompt）来引导视觉模型学习或者描述新的视觉概念，从而实现模型零样本传输到下游任务。
实验结果：模型可以轻松迁移到大多数任务，并且通常可以与完全监督的基线竞争，而无需任何数据集特定的训练。
代码链接：https://github.com/openai/CLIP
任务
图像文本理解：零样本迁移视觉任务
架构
从自然语言与图像配对的监督中学习感知
Image Encoder：提取图像特征，ResNet或者ViT，N个图像文本对得到N个图像特征
Text Encoder：提取文本特征，Transformer(GPT-2)，N个图像文本对得到N个文本特征

训练方法
训练数据：网络搜集的4亿图像文本对
训练阶段：
CLIP训练Image Encoder与Text Encoder学习多模态嵌入空间，最大化N*N矩阵对角线位置的余弦相似度（N个正样本），同时最小化其余位置的余弦相似度（N*N-N个负样本）
N*N矩阵的每一行对应一张图像，其中对角线位置是类别gt_label，将余弦相似度作为logits，使用交叉熵损失最大化对角线位置的余弦相似度
下游任务推理阶段：
prompt template：将图像的所有类变成一个句子，也就是将这些类别去替代 “A photo of a {object}” 中的 “{object}” ，以 “plane” 类为例，它就变成"A photo of a plane"，然后通过先前预训练好的 Text Encoder 就会得到1000个文本的特征。直接使用单词抽取文本特征也是可以的，但是因为在模型预训练时，与图像对应的都是句子，如果在推理的时候，把所有的文本都变成了单词，那这样就跟训练时看到的文本不太一样了，所以效果就会有所下降。此外，在推理时如何将单词变成句子也是有讲究的，作者也提出了 prompt engineering 和 prompt ensemble，而且不需要重新训练模型。
计算图像特征与文本特征余弦相似度，得到最大的配对，完成分类推理。
实验效果
在 ImageNet 上 监督训练收敛的ResNet 和 CLIP 效果相同，但在 ImageNetV2、ImageNet-R、ObjectNet、ImageNet Sketch、ImageNet-A上，ResNet 迁移的效果惨目忍睹，但对于 CLIP 来说，它的效果始终都非常好。
